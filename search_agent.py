"""SearchAgent sub-agent for systematic web research using code execution.

This module provides a research agent that generates and executes complete
Python programs to conduct research, replacing the multi-step tool calling
pattern with a single code generation and execution approach.

Current mode: LLM generates complete code -> execute_search_code() -> results
Previous mode: LLM decision -> tool1 -> LLM decision -> tool2 -> ... (multiple cycles)
"""

from deepagents import SubAgent
from code_executor import create_execute_search_code_tool
from tools import get_current_time, get_collected_summary


# Default maximum search rounds
DEFAULT_MAX_SEARCH_ROUNDS = 5


SEARCH_AGENT = SubAgent(
    name="search_agent",
    description=f"""ä¸“ä¸šçš„ç ”ç©¶ä»£ç†ï¼Œé€šè¿‡ç”Ÿæˆå¹¶æ‰§è¡Œå®Œæ•´çš„Pythonæœç´¢ç¨‹åºæ¥å®Œæˆç ”ç©¶ä»»åŠ¡ã€‚

æ ¸å¿ƒç‰¹æ€§ï¼š
- ğŸ–¥ï¸ ä»£ç æ‰§è¡Œæ¨¡å¼ï¼šç”Ÿæˆå®Œæ•´çš„Pythonç¨‹åºï¼Œä¸€æ¬¡æ€§æ‰§è¡Œæ‰€æœ‰æœç´¢é€»è¾‘
- ğŸ• å¼ºæ—¶æ•ˆæ€§ï¼šé»˜è®¤åªæœç´¢æœ€è¿‘ä¸€ä¸ªæœˆå†…çš„ä¿¡æ¯ï¼Œæ‹’ç»è¿‡æ—¶å†…å®¹
- ğŸ“Š ç»“æ„åŒ–è¾“å‡ºï¼šç”Ÿæˆå¸¦æ—¶é—´æ ‡æ³¨çš„ç ”ç©¶æŠ¥å‘Š
- ğŸ”„ åŠ¨æ€è°ƒæ•´ï¼šç¨‹åºå†…è‡ªä¸»å†³å®šæœç´¢ç­–ç•¥å’Œå¾ªç¯æ§åˆ¶
- ğŸ¤– è‡ªä¸»åæ€ï¼šä»£ç ä¸­åŒ…å«å®Œæ•´çš„åæ€å’Œå†³ç­–é€»è¾‘

é€‚ç”¨åœºæ™¯ï¼š
- éœ€è¦æœ€æ–°ä¿¡æ¯çš„ç ”ç©¶ä»»åŠ¡ï¼ˆæ–°é—»ã€å¸‚åœºåŠ¨æ€ã€æŠ€æœ¯è¿›å±•ï¼‰
- éœ€è¦ä»å¤šä¸ªè§’åº¦æœç´¢åŒä¸€ä¸»é¢˜
- éœ€è¦å¯¹æ¯”å¤šä¸ªæ¥æºçš„ä¿¡æ¯å¹¶è¿›è¡Œæ—¶æ•ˆæ€§éªŒè¯
- å¤æ‚ç ”ç©¶ä»»åŠ¡ï¼Œéœ€è¦å¤šè½®è¿­ä»£æœç´¢""",

    system_prompt=f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç ”ç©¶åŠ©æ‰‹ï¼Œé€šè¿‡ç”Ÿæˆå®Œæ•´çš„Pythonç¨‹åºæ¥å®Œæˆæœç´¢ä»»åŠ¡ã€‚

## ğŸ–¥ï¸ å·¥ä½œæ¨¡å¼

ä½ éœ€è¦ç”Ÿæˆä¸€æ®µå®Œæ•´çš„Pythonç¨‹åºä»£ç ï¼Œç„¶åè°ƒç”¨ `execute_search_code` å·¥å…·æ¥æ‰§è¡Œå®ƒã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼šå°†æ‰€æœ‰æœç´¢é€»è¾‘å†™å…¥ä¸€æ®µç¨‹åºï¼Œä¸€æ¬¡æ€§æ‰§è¡Œå®Œæˆï¼Œè€Œä¸æ˜¯å¤šæ¬¡è°ƒç”¨å·¥å…·ã€‚

---

## â°ã€é¦–è¦åŸåˆ™ã€‘æ—¶é—´æ„ŸçŸ¥

ç¨‹åºå¿…é¡»é¦–å…ˆè·å–å½“å‰æ—¶é—´ï¼Œæ‰€æœ‰åç»­å†³ç­–éƒ½åŸºäºæ­¤ï¼š

```python
# ç¬¬ä¸€æ­¥ï¼šè·å–å½“å‰æ—¶é—´
t = get_current_time()
print(f"æœç´¢å¼€å§‹: {{t['message']}}")
print(f"ä»Šå¤©æ˜¯ {{t['date']}} {{t['weekday']}}")
```

---

## ğŸ“ ç¨‹åºæ¨¡æ¿

ä»¥ä¸‹æ˜¯æ ‡å‡†çš„æœç´¢ç¨‹åºæ¨¡æ¿ï¼Œè¯·æ ¹æ®å…·ä½“ä»»åŠ¡è°ƒæ•´ï¼š

```python
# ============================================
# ç¬¬1æ­¥ï¼šè·å–å½“å‰æ—¶é—´ï¼ˆå¿…é¡»é¦–å…ˆæ‰§è¡Œï¼‰
# ============================================
t = get_current_time()
print(f"æœç´¢å¼€å§‹: {{t['message']}}")

# ============================================
# ç¬¬2æ­¥ï¼šåˆå§‹åŒ–æœç´¢ä¼šè¯
# ============================================
init_search_session(max_search_rounds=5)
set_search_task(
    task="[å…·ä½“çš„ç ”ç©¶ä»»åŠ¡æè¿°]",
    required_info_types=["news", "data", "analysis"],  # æ ¹æ®ä»»åŠ¡è°ƒæ•´
    min_sources=3,
    time_sensitivity="oneMonth"
)

# ============================================
# ç¬¬3æ­¥ï¼šæœç´¢-åæ€å¾ªç¯
# ============================================
while True:
    # è·å–å½“å‰çŠ¶æ€
    status = get_search_status()
    print(f"\\n=== ç¬¬{{status['current_round'] + 1}}è½®æœç´¢ ===")
    print(f"å‰©ä½™è½®æ•°: {{status['remaining_rounds']}}")

    # æ‰§è¡Œæœç´¢ï¼ˆæ ¹æ®ä»»åŠ¡è°ƒæ•´æŸ¥è¯¢å’Œå‚æ•°ï¼‰
    results = web_search(
        query="[æœç´¢å…³é”®è¯]",
        max_results=5,
        freshness="oneMonth"  # æ ¹æ®æ—¶æ•ˆéœ€æ±‚è°ƒæ•´
    )

    # å¤„ç†æœç´¢ç»“æœï¼ˆå…¼å®¹Tavilyå’ŒBochaAIä¸¤ç§æ ¼å¼ï¼‰
    pages = []
    if results:
        # Tavilyæ ¼å¼: results['results']
        if 'results' in results:
            pages = results['results']
        # BochaAIæ ¼å¼: results['data']['webPages']['value']
        elif 'data' in results and 'webPages' in results['data']:
            pages = results['data']['webPages']['value']

    if pages:
        print(f"æ‰¾åˆ° {{len(pages)}} ä¸ªç»“æœ")

        for page in pages[:3]:  # å¤„ç†å‰3ä¸ªç»“æœ
            # å…¼å®¹ä¸¤ç§æ ¼å¼çš„å­—æ®µå
            title = page.get('title', page.get('name', 'N/A'))
            url = page.get('url', page.get('link', ''))
            snippet = page.get('content', page.get('snippet', page.get('summary', '')))
            print(f"  - {{title}}")

            # è¯»å–ç½‘é¡µè¯¦ç»†å†…å®¹ï¼Œä½¿ç”¨LLMæå–æ ‡é¢˜ã€å‘å¸ƒæ—¶é—´å’Œä¸»è¦å†…å®¹
            try:
                page_data = web_read(url)  # è¿”å› dict: title, publish_time, content, raw_content, url
                extracted_title = page_data.get('title') or title
                publish_time = page_data.get('publish_time')
                main_content = page_data.get('content', snippet)

                print(f"    æ ‡é¢˜: {{extracted_title}}")
                print(f"    å‘å¸ƒæ—¶é—´: {{publish_time or 'æœªæ‰¾åˆ°'}}")
                print(f"    å†…å®¹é•¿åº¦: {{len(main_content)}} å­—ç¬¦")
            except Exception as e:
                publish_time = page.get('published_date') or page.get('datePublished')
                main_content = snippet
                print(f"    è¯»å–å¤±è´¥: {{str(e)[:50]}}")

            # æ”¶é›†æœ‰ä»·å€¼çš„ä¿¡æ¯
            add_collected_info(
                content=main_content,
                source=url,
                publish_time=publish_time,
                relevance=0.8,
                category="main"
            )

    # è®°å½•æœç´¢
    record_search_result(
        query="[æœç´¢å…³é”®è¯]",
        freshness="oneMonth",
        total_results=len(pages) if pages else 0,
        valid_results=min(3, len(pages)) if pages else 0,
        notes="æœç´¢ç»“æœæ¦‚è¿°"
    )

    # åæ€è¯„ä¼°
    coverage = reflect_on_coverage(
        task_description="[åŸå§‹ä»»åŠ¡]",
        covered_aspects=["å·²è¦†ç›–æ–¹é¢1", "å·²è¦†ç›–æ–¹é¢2"],
        missing_aspects=["ç¼ºå¤±æ–¹é¢1"]
    )
    print(f"è¦†ç›–åˆ†æ: {{coverage}}")

    # å†³ç­–æ˜¯å¦ç»§ç»­
    decision = should_continue_searching(
        task_complete=False  # ä»»åŠ¡å®Œæˆæ—¶è®¾ä¸ºTrue
    )
    print(f"å†³ç­–: {{decision['reason']}}")

    if not decision['should_continue']:
        print("æœç´¢ç»“æŸ")
        break

# ============================================
# ç¬¬4æ­¥ï¼šè¾“å‡ºç»“æœæ‘˜è¦
# ============================================
print("\\n" + "="*50)
print("æœç´¢å®Œæˆï¼")
summary = get_collected_summary()
print(f"æ”¶é›†ä¿¡æ¯: {{summary['total_items']}} æ¡")
print(f"ç‹¬ç«‹æ¥æº: {{summary['unique_sources']}} ä¸ª")
print(f"ä¿¡æ¯ç±»åˆ«: {{summary['categories']}}")

# è®¾ç½®è¿”å›ç»“æœ
result = summary
```

---

## ğŸ› ï¸ å¯ç”¨å‡½æ•°

åœ¨ç”Ÿæˆçš„ä»£ç ä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ï¼š

### æ—¶é—´å·¥å…·
- `get_current_time()` - è·å–å½“å‰ç³»ç»Ÿæ—¶é—´ï¼Œè¿”å›datetime, date, yearç­‰

### æœç´¢å·¥å…·
- `web_search(query, max_results=5, freshness="noLimit", topic="general")` - æ‰§è¡Œç½‘ç»œæœç´¢
- `web_read(url)` - è¯»å–ç½‘é¡µå¹¶ä½¿ç”¨LLMæå–ç»“æ„åŒ–ä¿¡æ¯ï¼Œè¿”å› dict:
  - `title`: ç½‘é¡µæ ‡é¢˜
  - `publish_time`: å‘å¸ƒæ—¶é—´ (YYYY-MM-DD)
  - `content`: ä¸»è¦å†…å®¹æ‘˜è¦
  - `raw_content`: åŸå§‹ç½‘é¡µå†…å®¹
  - `url`: åŸå§‹URL

### ä¼šè¯ç®¡ç†
- `init_search_session(max_search_rounds=5)` - åˆå§‹åŒ–æœç´¢ä¼šè¯
- `set_search_task(task, required_info_types, min_sources, time_sensitivity)` - è®¾ç½®ä»»åŠ¡ç›®æ ‡
- `get_search_status()` - è·å–å½“å‰æœç´¢çŠ¶æ€
- `get_search_history()` - è·å–æœç´¢å†å²

### ä¿¡æ¯æ”¶é›†
- `record_search_result(query, freshness, total_results, valid_results, notes)` - è®°å½•æœç´¢ç»“æœ
- `add_collected_info(content, source, publish_time, relevance, category)` - ä¿å­˜æ”¶é›†çš„ä¿¡æ¯
- `get_collected_summary()` - è·å–å·²æ”¶é›†ä¿¡æ¯çš„æ‘˜è¦

### åæ€å·¥å…·
- `reflect_on_coverage(task_description, covered_aspects, missing_aspects)` - è¯„ä¼°è¦†ç›–æƒ…å†µ
- `evaluate_search_quality(dimension)` - è¯„ä¼°è´¨é‡ï¼ˆç»´åº¦ï¼šcompleteness/timeliness/relevance/diversity/credibilityï¼‰
- `should_continue_searching(task_complete, reasons_to_stop)` - å†³å®šæ˜¯å¦ç»§ç»­æœç´¢

### å…¶ä»–
- `print()` - è¾“å‡ºä¿¡æ¯
- `json` - JSONæ¨¡å—
- `re` - æ­£åˆ™è¡¨è¾¾å¼æ¨¡å—ï¼ˆç”¨äºæå–ç½‘é¡µä¸­çš„å‘å¸ƒæ—¶é—´ç­‰ï¼‰

---

## â° æ—¶æ•ˆæ€§è§„åˆ™

åœ¨ä»£ç ä¸­è®¾ç½® `freshness` å‚æ•°æ—¶ï¼Œå¿…é¡»æ ¹æ®ä¿¡æ¯ç±»å‹é€‰æ‹©ï¼š

| ä¿¡æ¯ç±»å‹ | freshness | è¯´æ˜ |
|---------|-----------|------|
| è‚¡ç¥¨è¡Œæƒ…ã€çªå‘æ–°é—» | `"oneDay"` | å®æ—¶æ€§è¦æ±‚æœ€é«˜ |
| æŠ€æœ¯åŠ¨æ€ã€äº§å“å‘å¸ƒ | `"oneWeek"` | ä¸€å‘¨å†…çš„ä¿¡æ¯ |
| è¡Œä¸šåˆ†æã€ç ”ç©¶æŠ¥å‘Š | `"oneMonth"` | **é»˜è®¤é€‰æ‹©** |
| é•¿æœŸè¶‹åŠ¿ã€å†å²å¯¹æ¯” | `"oneYear"` | éœ€è¦å†å²æ•°æ®æ—¶ |
| ç”¨æˆ·æ˜ç¡®è¦æ±‚å†å² | `"noLimit"` | å¿…é¡»æœ‰æ˜ç¡®è¯´æ˜ |

---

## ğŸ“¤ è¾“å‡ºè¦æ±‚

ç”Ÿæˆçš„ä»£ç åº”è¯¥ï¼š

1. **æ¸…æ™°çš„è¿›åº¦è¾“å‡º**ï¼šä½¿ç”¨print()æ˜¾ç¤ºæœç´¢è¿›åº¦
2. **è®¾ç½®resultå˜é‡**ï¼šå°†æœ€ç»ˆç»“æœèµ‹å€¼ç»™resultå˜é‡
3. **å®Œæ•´çš„æœç´¢æŠ¥å‘Š**ï¼šè¾“å‡ºç»“æ„åŒ–çš„æœç´¢ç»“æœ

æœ€ç»ˆè¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
```
æœç´¢å¼€å§‹: 2025å¹´2æœˆ16æ—¥ ...

=== ç¬¬1è½®æœç´¢ ===
å‰©ä½™è½®æ•°: 4
æ‰¾åˆ° 10 ä¸ªç»“æœ
  - æ ‡é¢˜1
  - æ ‡é¢˜2
...

æœç´¢å®Œæˆï¼
æ”¶é›†ä¿¡æ¯: 15 æ¡
ç‹¬ç«‹æ¥æº: 8 ä¸ª
```

---

## âš ï¸ é‡è¦æé†’

1. **å¿…é¡»é¦–å…ˆè°ƒç”¨get_current_time()** - è¿™æ˜¯æ—¶é—´æ„ŸçŸ¥çš„åŸºç¡€
2. **ä½¿ç”¨whileå¾ªç¯æ§åˆ¶æœç´¢è½®æ•°** - é…åˆshould_continue_searching()
3. **æ¯è½®æœç´¢ååæ€** - ä½¿ç”¨reflect_on_coverage()è¯„ä¼°è¿›åº¦
4. **è®¾ç½®resultå˜é‡** - ä¾¿äºè¿”å›ç»“æ„åŒ–ç»“æœ
5. **ç¦æ­¢ä½¿ç”¨importè¯­å¥** - æ‰€æœ‰å·¥å…·å·²é¢„ç½®
6. **ç¦æ­¢å®šä¹‰ç±»** - åªä½¿ç”¨å‡½æ•°å¼ç¼–ç¨‹

---

## è°ƒç”¨æ–¹å¼

ç”Ÿæˆä»£ç åï¼Œä½¿ç”¨execute_search_codeå·¥å…·æ‰§è¡Œï¼š

```
execute_search_code(code="ä½ çš„å®Œæ•´ç¨‹åºä»£ç ")
```

å·¥å…·å°†è¿”å›ï¼š
- success: æ˜¯å¦æˆåŠŸæ‰§è¡Œ
- output: æ‰€æœ‰print()è¾“å‡º
- result: resultå˜é‡çš„å€¼
- error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰""",

    tools=[
        create_execute_search_code_tool(),  # æ ¸å¿ƒå·¥å…·ï¼šæ‰§è¡Œç”Ÿæˆçš„ä»£ç 
        get_current_time,                    # è¾…åŠ©å·¥å…·ï¼šç›´æ¥è·å–æ—¶é—´
        get_collected_summary,               # è¾…åŠ©å·¥å…·ï¼šè·å–æ”¶é›†ç»“æœ
    ],
)

__all__ = ["SEARCH_AGENT", "DEFAULT_MAX_SEARCH_ROUNDS"]


def create_search_agent():
    """Create and return a deep agent with search_agent capabilities."""
    import os
    from datetime import datetime
    from deepagents import create_deep_agent
    from langchain_openai import ChatOpenAI

    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    model = ChatOpenAI(
        temperature=0.3,
        model="glm-4.7",
        openai_api_key=os.getenv("ZHIPUAI_API_KEY"),
        openai_api_base="https://open.bigmodel.cn/api/paas/v4/"
    )

    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æœç´¢åŠ©æ‰‹ï¼Œè´Ÿè´£è°ƒç”¨ search_agent å®Œæˆæœç´¢ä»»åŠ¡ã€‚

## â° å½“å‰ç³»ç»Ÿæ—¶é—´

**ç°åœ¨çš„æ—¶é—´æ˜¯: {current_time}**

---

## æ ¸å¿ƒä»»åŠ¡

å½“ç”¨æˆ·æä¾›æœç´¢æŸ¥è¯¢æ—¶ï¼Œè°ƒç”¨ search_agent æ‰§è¡Œæœç´¢ä»»åŠ¡ã€‚
search_agent ä¼šç”Ÿæˆå¹¶æ‰§è¡Œå®Œæ•´çš„ Python æœç´¢ç¨‹åºæ¥å®Œæˆç ”ç©¶ä»»åŠ¡ã€‚

## å·¥ä½œæµç¨‹

1. æ¥æ”¶ç”¨æˆ·çš„æœç´¢æŸ¥è¯¢
2. è°ƒç”¨ search_agentï¼Œæ˜ç¡®å‘ŠçŸ¥æœç´¢ç›®æ ‡å’Œæ—¶æ•ˆæ€§è¦æ±‚
3. ç­‰å¾… search_agent è¿”å›æœç´¢ç»“æœ
4. å‘ç”¨æˆ·å±•ç¤ºæœç´¢ç»“æœ

## æ³¨æ„äº‹é¡¹

- å¯¹äºæ—¶æ•ˆæ€§è¦æ±‚é«˜çš„ä¿¡æ¯ï¼ˆæ–°é—»ã€è‚¡ä»·ï¼‰ï¼Œè¦æ±‚ search_agent ä½¿ç”¨ oneDay æˆ– oneWeek
- å¯¹äºä¸€èˆ¬æ€§ç ”ç©¶ï¼Œä½¿ç”¨ oneMonth
- æ˜ç¡®å‘ŠçŸ¥ search_agent éœ€è¦æœç´¢çš„å†…å®¹å’ŒæœŸæœ›çš„ç»“æœæ•°é‡
"""

    agent = create_deep_agent(
        model=model,
        subagents=[SEARCH_AGENT],
        system_prompt=system_prompt,
        debug=True
    )

    return agent


if __name__ == "__main__":
    import sys

    # Get query from command line arguments
    if len(sys.argv) < 2:
        print("Usage: python search_agent.py <search query>")
        print("Example: python search_agent.py 'AI trends 2025'")
        sys.exit(1)

    query = " ".join(sys.argv[1:])

    # Print search task info
    print("=" * 60)
    print("ğŸ“‹ æœç´¢ä»»åŠ¡")
    print("=" * 60)
    print(f"  æŸ¥è¯¢å†…å®¹: {query}")
    print(f"  æ‰§è¡Œæ¨¡å¼: Agent ä»£ç ç”Ÿæˆ + æ‰§è¡Œ")
    print("=" * 60)
    print()

    # Create agent and execute search
    print("ğŸš€ å¯åŠ¨ Search Agent...\n")

    agent = create_search_agent()

    # Prepare the query for the agent
    agent_query = f"""è¯·æ‰§è¡Œä»¥ä¸‹æœç´¢ä»»åŠ¡ï¼š

æœç´¢æŸ¥è¯¢: {query}

è¦æ±‚ï¼š
1. ä½¿ç”¨ execute_search_code å·¥å…·ç”Ÿæˆå¹¶æ‰§è¡Œæœç´¢ä»£ç 
2. æ—¶æ•ˆæ€§è¦æ±‚: oneMonth (ä¸€ä¸ªæœˆå†…)
3. è¿”å› 5 æ¡ç»“æœ
4. å±•ç¤ºæ¯æ¡ç»“æœçš„æ ‡é¢˜ã€URLã€å‘å¸ƒæ—¶é—´å’Œæ‘˜è¦

è¯·å¼€å§‹æœç´¢å¹¶è¿”å›ç»“æœã€‚
"""

    # Stream the agent execution
    for chunk in agent.stream(
        {"messages": [{"role": "user", "content": agent_query}]},
        stream_mode="updates"
    ):
        for node_name, node_output in chunk.items():
            if node_output is not None and "messages" in node_output:
                messages = node_output["messages"]
                if hasattr(messages, 'value'):
                    messages = messages.value
                for msg in messages:
                    content = getattr(msg, 'content', str(msg))
                    if content:
                        print(f"[{node_name}] {content}")
            elif node_output is not None:
                print(f"[{node_name}] {node_output}")

    print()
    print("=" * 60)
    print("âœ… æœç´¢å®Œæˆ")
    print("=" * 60)
