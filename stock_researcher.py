"""Stock Researcher Agent - ä¸ªè‚¡ç ”ç©¶å‘˜

This module provides a specialized agent for researching individual stocks from a
value investment perspective. It generates and executes Python code to conduct
systematic research across six dimensions.
"""

from deepagents import SubAgent
from code_executor import execute_search_code
from tools import get_current_time, get_collected_summary


# è‚¡ç¥¨ç ”ç©¶ä»£ç æ¨¡æ¿ - å…­ç»´åº¦åˆ†æ
STOCK_RESEARCH_CODE_TEMPLATE = '''
# ============================================
# è‚¡ç¥¨ç ”ç©¶ç¨‹åº - å…­ç»´åº¦ä»·å€¼åˆ†æ
# è‚¡ç¥¨: {stock_name} ({stock_code})
# ============================================

# ç¬¬1æ­¥ï¼šè·å–å½“å‰æ—¶é—´ï¼ˆå¿…é¡»é¦–å…ˆæ‰§è¡Œï¼‰
t = get_current_time()
print("=" * 60)
print(f"ğŸ“Š è‚¡ç¥¨ç ”ç©¶æŠ¥å‘Š")
print(f"   è‚¡ç¥¨: {stock_name} ({stock_code})")
print(f"   æ—¶é—´: {{t['message']}}")
print("=" * 60)

# ç¬¬2æ­¥ï¼šåˆå§‹åŒ–æœç´¢ä¼šè¯
init_search_session(max_search_rounds=8)
set_search_task(
    task="{stock_name}({stock_code})æŠ•èµ„ä»·å€¼ç ”ç©¶",
    required_info_types=["è´¢åŠ¡æ•°æ®", "å…¬å¸æ–°é—»", "è‚¡ä»·ä¼°å€¼", "é«˜ç®¡åŠ¨æ€", "è¡Œä¸šåˆ†æ", "æœºæ„è§‚ç‚¹"],
    min_sources=6,
    time_sensitivity="oneMonth"
)

# å­˜å‚¨å„ç»´åº¦ç ”ç©¶ç»“æœ
research_data = {{
    "stock_name": "{stock_name}",
    "stock_code": "{stock_code}",
    "research_time": t['datetime'],
    "dimensions": {{}}
}}

# ============================================
# ç»´åº¦ä¸€ï¼šå…¬å¸åŸºæœ¬é¢åˆ†æ
# ============================================
print("\\nğŸ“Œ [ç»´åº¦1/6] å…¬å¸åŸºæœ¬é¢åˆ†æ")
print("-" * 40)

results_1 = web_search(
    query="{stock_name} è´¢åŠ¡æŠ¥å‘Š è¥æ”¶ åˆ©æ¶¦ ROE",
    max_results=5,
    freshness="oneMonth"
)

pages_1 = results_1.get('results', []) if 'results' in results_1 else results_1.get('data', {{}}).get('webPages', {{}}).get('value', [])
print(f"æœç´¢åˆ° {{len(pages_1)}} æ¡åŸºæœ¬é¢ä¿¡æ¯")

fundamentals = []
for page in pages_1[:3]:
    title = page.get('title', page.get('name', 'N/A'))
    url = page.get('url', page.get('link', ''))
    snippet = page.get('content', page.get('snippet', page.get('summary', '')))
    print(f"  - {{title}}")

    try:
        content = web_read(url)
        # æå–å…³é”®è´¢åŠ¡æŒ‡æ ‡
        main_content = content[:2000]
        fundamentals.append({{"title": title, "url": url, "content": main_content}})
        add_collected_info(content=main_content, source=url, relevance=0.9, category="fundamentals")
    except Exception as e:
        fundamentals.append({{"title": title, "url": url, "snippet": snippet}})
        add_collected_info(content=snippet, source=url, relevance=0.7, category="fundamentals")

record_search_result(
    query="{stock_name} è´¢åŠ¡æŠ¥å‘Š",
    freshness="oneMonth",
    total_results=len(pages_1),
    valid_results=len(fundamentals),
    notes="åŸºæœ¬é¢ä¿¡æ¯æ”¶é›†"
)
research_data["dimensions"]["fundamentals"] = fundamentals

# ============================================
# ç»´åº¦äºŒï¼šæœ€æ–°å…¬å¸æ–°é—»
# ============================================
print("\\nğŸ“Œ [ç»´åº¦2/6] æœ€æ–°å…¬å¸æ–°é—»")
print("-" * 40)

results_2 = web_search(
    query="{stock_name} æœ€æ–°æ¶ˆæ¯ æ–°é—»",
    max_results=5,
    freshness="oneWeek"
)

pages_2 = results_2.get('results', []) if 'results' in results_2 else results_2.get('data', {{}}).get('webPages', {{}}).get('value', [])
print(f"æœç´¢åˆ° {{len(pages_2)}} æ¡æ–°é—»")

news_items = []
for page in pages_2[:3]:
    title = page.get('title', page.get('name', 'N/A'))
    url = page.get('url', page.get('link', ''))
    snippet = page.get('content', page.get('snippet', page.get('summary', '')))
    print(f"  - {{title}}")

    try:
        content = web_read(url)
        main_content = content[:2000]
        news_items.append({{"title": title, "url": url, "content": main_content}})
        add_collected_info(content=main_content, source=url, relevance=0.85, category="news")
    except Exception as e:
        news_items.append({{"title": title, "url": url, "snippet": snippet}})
        add_collected_info(content=snippet, source=url, relevance=0.6, category="news")

record_search_result(
    query="{stock_name} æœ€æ–°æ¶ˆæ¯",
    freshness="oneWeek",
    total_results=len(pages_2),
    valid_results=len(news_items),
    notes="æœ€æ–°æ–°é—»æ”¶é›†"
)
research_data["dimensions"]["news"] = news_items

# ============================================
# ç»´åº¦ä¸‰ï¼šè‚¡ä»·ä¼°å€¼åˆ†æ
# ============================================
print("\\nğŸ“Œ [ç»´åº¦3/6] è‚¡ä»·ä¼°å€¼åˆ†æ")
print("-" * 40)

results_3 = web_search(
    query="{stock_name} è‚¡ä»· PE ä¼°å€¼ PB",
    max_results=5,
    freshness="oneWeek"
)

pages_3 = results_3.get('results', []) if 'results' in results_3 else results_3.get('data', {{}}).get('webPages', {{}}).get('value', [])
print(f"æœç´¢åˆ° {{len(pages_3)}} æ¡ä¼°å€¼ä¿¡æ¯")

valuation_data = []
for page in pages_3[:3]:
    title = page.get('title', page.get('name', 'N/A'))
    url = page.get('url', page.get('link', ''))
    snippet = page.get('content', page.get('snippet', page.get('summary', '')))
    print(f"  - {{title}}")

    try:
        content = web_read(url)
        main_content = content[:2000]
        valuation_data.append({{"title": title, "url": url, "content": main_content}})
        add_collected_info(content=main_content, source=url, relevance=0.9, category="valuation")
    except Exception as e:
        valuation_data.append({{"title": title, "url": url, "snippet": snippet}})
        add_collected_info(content=snippet, source=url, relevance=0.7, category="valuation")

record_search_result(
    query="{stock_name} è‚¡ä»·ä¼°å€¼",
    freshness="oneWeek",
    total_results=len(pages_3),
    valid_results=len(valuation_data),
    notes="ä¼°å€¼æ•°æ®æ”¶é›†"
)
research_data["dimensions"]["valuation"] = valuation_data

# ============================================
# ç»´åº¦å››ï¼šé«˜ç®¡åŠ¨æ€
# ============================================
print("\\nğŸ“Œ [ç»´åº¦4/6] é«˜ç®¡åŠ¨æ€")
print("-" * 40)

results_4 = web_search(
    query="{stock_name} é«˜ç®¡å˜åŠ¨ è‘£äº‹é•¿ æ€»ç»ç† å¢æŒå‡æŒ",
    max_results=5,
    freshness="oneMonth"
)

pages_4 = results_4.get('results', []) if 'results' in results_4 else results_4.get('data', {{}}).get('webPages', {{}}).get('value', [])
print(f"æœç´¢åˆ° {{len(pages_4)}} æ¡é«˜ç®¡ä¿¡æ¯")

management_info = []
for page in pages_4[:3]:
    title = page.get('title', page.get('name', 'N/A'))
    url = page.get('url', page.get('link', ''))
    snippet = page.get('content', page.get('snippet', page.get('summary', '')))
    print(f"  - {{title}}")

    try:
        content = web_read(url)
        main_content = content[:2000]
        management_info.append({{"title": title, "url": url, "content": main_content}})
        add_collected_info(content=main_content, source=url, relevance=0.8, category="management")
    except Exception as e:
        management_info.append({{"title": title, "url": url, "snippet": snippet}})
        add_collected_info(content=snippet, source=url, relevance=0.6, category="management")

record_search_result(
    query="{stock_name} é«˜ç®¡åŠ¨æ€",
    freshness="oneMonth",
    total_results=len(pages_4),
    valid_results=len(management_info),
    notes="é«˜ç®¡ä¿¡æ¯æ”¶é›†"
)
research_data["dimensions"]["management"] = management_info

# ============================================
# ç»´åº¦äº”ï¼šè¡Œä¸šè¶‹åŠ¿
# ============================================
print("\\nğŸ“Œ [ç»´åº¦5/6] è¡Œä¸šè¶‹åŠ¿")
print("-" * 40)

results_5 = web_search(
    query="{industry} è¡Œä¸šè¶‹åŠ¿ å‰æ™¯ æ™¯æ°”åº¦",
    max_results=5,
    freshness="oneMonth"
)

pages_5 = results_5.get('results', []) if 'results' in results_5 else results_5.get('data', {{}}).get('webPages', {{}}).get('value', [])
print(f"æœç´¢åˆ° {{len(pages_5)}} æ¡è¡Œä¸šä¿¡æ¯")

industry_info = []
for page in pages_5[:3]:
    title = page.get('title', page.get('name', 'N/A'))
    url = page.get('url', page.get('link', ''))
    snippet = page.get('content', page.get('snippet', page.get('summary', '')))
    print(f"  - {{title}}")

    try:
        content = web_read(url)
        main_content = content[:2000]
        industry_info.append({{"title": title, "url": url, "content": main_content}})
        add_collected_info(content=main_content, source=url, relevance=0.75, category="industry")
    except Exception as e:
        industry_info.append({{"title": title, "url": url, "snippet": snippet}})
        add_collected_info(content=snippet, source=url, relevance=0.5, category="industry")

record_search_result(
    query="{industry} è¡Œä¸šè¶‹åŠ¿",
    freshness="oneMonth",
    total_results=len(pages_5),
    valid_results=len(industry_info),
    notes="è¡Œä¸šåˆ†ææ”¶é›†"
)
research_data["dimensions"]["industry"] = industry_info

# ============================================
# ç»´åº¦å…­ï¼šæœºæ„è§‚ç‚¹
# ============================================
print("\\nğŸ“Œ [ç»´åº¦6/6] æœºæ„è§‚ç‚¹")
print("-" * 40)

results_6 = web_search(
    query="{stock_name} åˆ¸å•†ç ”æŠ¥ ç›®æ ‡ä»· è¯„çº§ æœºæ„è°ƒç ”",
    max_results=5,
    freshness="oneMonth"
)

pages_6 = results_6.get('results', []) if 'results' in results_6 else results_6.get('data', {{}}).get('webPages', {{}}).get('value', [])
print(f"æœç´¢åˆ° {{len(pages_6)}} æ¡æœºæ„è§‚ç‚¹")

analyst_views = []
for page in pages_6[:3]:
    title = page.get('title', page.get('name', 'N/A'))
    url = page.get('url', page.get('link', ''))
    snippet = page.get('content', page.get('snippet', page.get('summary', '')))
    print(f"  - {{title}}")

    try:
        content = web_read(url)
        main_content = content[:2000]
        analyst_views.append({{"title": title, "url": url, "content": main_content}})
        add_collected_info(content=main_content, source=url, relevance=0.85, category="analyst")
    except Exception as e:
        analyst_views.append({{"title": title, "url": url, "snippet": snippet}})
        add_collected_info(content=snippet, source=url, relevance=0.6, category="analyst")

record_search_result(
    query="{stock_name} åˆ¸å•†ç ”æŠ¥",
    freshness="oneMonth",
    total_results=len(pages_6),
    valid_results=len(analyst_views),
    notes="æœºæ„è§‚ç‚¹æ”¶é›†"
)
research_data["dimensions"]["analyst"] = analyst_views

# ============================================
# ç”Ÿæˆç ”ç©¶æ‘˜è¦
# ============================================
print("\\n" + "=" * 60)
print("ğŸ“Š ç ”ç©¶å®Œæˆï¼")
print("=" * 60)

summary = get_collected_summary()
print(f"æ”¶é›†ä¿¡æ¯: {{summary['total_items']}} æ¡")
print(f"ç‹¬ç«‹æ¥æº: {{summary['unique_sources']}} ä¸ª")
print(f"ä¿¡æ¯ç±»åˆ«: {{summary['categories']}}")

# åæ€è¦†ç›–åº¦
coverage = reflect_on_coverage(
    task_description="{stock_name}({stock_code})æŠ•èµ„ä»·å€¼ç ”ç©¶",
    covered_aspects=["åŸºæœ¬é¢", "æ–°é—»", "ä¼°å€¼", "é«˜ç®¡", "è¡Œä¸š", "æœºæ„è§‚ç‚¹"],
    missing_aspects=[]
)
print(f"\\nè¦†ç›–åº¦è¯„ä¼°: {{coverage['recommendations']}}")

# è®¾ç½®è¿”å›ç»“æœ
research_data["summary"] = summary
research_data["coverage"] = coverage
result = research_data

print("\\nâœ… ç ”ç©¶æ•°æ®å·²å‡†å¤‡å®Œæˆï¼Œå¯ä»¥ç”ŸæˆæŠ•èµ„åˆ†ææŠ¥å‘Š")
'''


def research_stock(stock_name: str, stock_code: str = "", industry: str = "") -> dict:
    """æ‰§è¡Œè‚¡ç¥¨ç ”ç©¶å¹¶è¿”å›ç ”ç©¶æ•°æ®ã€‚

    Args:
        stock_name: è‚¡ç¥¨åç§°ï¼ˆå¦‚"è´µå·èŒ…å°"ï¼‰
        stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚"600519"ï¼‰ï¼Œå¯é€‰
        industry: æ‰€å±è¡Œä¸šï¼ˆå¦‚"ç™½é…’"ï¼‰ï¼Œå¯é€‰

    Returns:
        åŒ…å«å…­ä¸ªç»´åº¦ç ”ç©¶æ•°æ®çš„å­—å…¸
    """
    # ç”Ÿæˆç ”ç©¶ä»£ç 
    code = STOCK_RESEARCH_CODE_TEMPLATE.format(
        stock_name=stock_name,
        stock_code=stock_code or stock_name,
        industry=industry or f"{stock_name}æ‰€å±è¡Œä¸š"
    )

    # æ‰§è¡Œç ”ç©¶ä»£ç 
    execution_result = execute_search_code(code, timeout=180)

    return {
        "stock_name": stock_name,
        "stock_code": stock_code,
        "execution": execution_result,
        "research_data": execution_result.get("result"),
    }


def create_stock_research_code(stock_name: str, stock_code: str = "", industry: str = "") -> str:
    """ç”Ÿæˆè‚¡ç¥¨ç ”ç©¶ä»£ç å­—ç¬¦ä¸²ã€‚

    Args:
        stock_name: è‚¡ç¥¨åç§°
        stock_code: è‚¡ç¥¨ä»£ç 
        industry: æ‰€å±è¡Œä¸š

    Returns:
        å¯æ‰§è¡Œçš„Pythonç ”ç©¶ä»£ç 
    """
    return STOCK_RESEARCH_CODE_TEMPLATE.format(
        stock_name=stock_name,
        stock_code=stock_code or stock_name,
        industry=industry or f"{stock_name}æ‰€å±è¡Œä¸š"
    )


STOCK_RESEARCHER = SubAgent(
    name="stock_researcher",
    description="""ä¸“ä¸šçš„ä¸ªè‚¡ç ”ç©¶å‘˜ä»£ç†ï¼Œä»ä»·å€¼æŠ•èµ„è§’åº¦å¯¹ä¸ªè‚¡è¿›è¡Œæ·±å…¥åˆ†æã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
- ğŸ“Š å…¬å¸åŸºæœ¬é¢åˆ†æï¼šè´¢åŠ¡æ•°æ®ã€ä¸šåŠ¡æ¨¡å¼ã€ç«äº‰ä¼˜åŠ¿
- ğŸ“° æœ€æ–°æ–°é—»è¿½è¸ªï¼šå…¬å¸åŠ¨æ€ã€é‡å¤§å…¬å‘Šã€å¸‚åœºæƒ…ç»ª
- ğŸ“ˆ è‚¡ä»·èµ°åŠ¿åˆ†æï¼šè¿‘æœŸè¡¨ç°ã€æŠ€æœ¯æŒ‡æ ‡ã€æˆäº¤é‡å˜åŒ–
- ğŸ‘” é«˜ç®¡åŠ¨æ€ç›‘æ§ï¼šç®¡ç†å±‚å˜åŠ¨ã€è‚¡æƒäº¤æ˜“ã€æˆ˜ç•¥å†³ç­–
- ğŸ­ è¡Œä¸šè¶‹åŠ¿ç ”åˆ¤ï¼šè¡Œä¸šæ™¯æ°”åº¦ã€æ”¿ç­–å½±å“ã€ç«äº‰æ ¼å±€
- ğŸ¦ æœºæ„è§‚ç‚¹æ±‡æ€»ï¼šåˆ¸å•†ç ”æŠ¥ã€ç›®æ ‡ä»·ã€æœºæ„è°ƒç ”

å·¥ä½œæ¨¡å¼ï¼š
- ç”Ÿæˆå®Œæ•´çš„å…­ç»´åº¦ç ”ç©¶ä»£ç 
- é€šè¿‡ execute_search_code ä¸€æ¬¡æ€§æ‰§è¡Œ
- é«˜æ•ˆå®Œæˆå¤šç»´åº¦ä¿¡æ¯æ”¶é›†

è¾“å‡ºç»“æœï¼š
- æœªæ¥ä¸‰ä¸ªæœˆæŠ•èµ„æ½œåŠ›è¯„ä¼°
- é£é™©å› ç´ åˆ†æ
- æŠ•èµ„å»ºè®®ï¼ˆä¹°å…¥/æŒæœ‰/è§‚å¯Ÿ/å›é¿ï¼‰

é€‚ç”¨åœºæ™¯ï¼š
- ç”¨æˆ·è¾“å…¥ä¸ªè‚¡åç§°ï¼ˆå¦‚"è´µå·èŒ…å°"ï¼‰æˆ–ä»£ç ï¼ˆå¦‚"600519"ï¼‰
- éœ€è¦ä»ä»·å€¼æŠ•èµ„è§’åº¦è¯„ä¼°ä¸ªè‚¡
- éœ€è¦ç»¼åˆå¤šç»´åº¦ä¿¡æ¯è¿›è¡ŒæŠ•èµ„å†³ç­–""",

    system_prompt="""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ä»·å€¼æŠ•èµ„ç ”ç©¶å‘˜ï¼Œéµå¾ªå·´è²ç‰¹çš„æŠ•èµ„ç†å¿µï¼Œä¸“æ³¨äºä»åŸºæœ¬é¢è§’åº¦åˆ†æä¸ªè‚¡çš„æŠ•èµ„ä»·å€¼ã€‚

## â°ã€é¦–è¦ä»»åŠ¡ã€‘è·å–å½“å‰æ—¶é—´

**åœ¨å¼€å§‹ä»»ä½•ç ”ç©¶ä¹‹å‰ï¼Œå…ˆè·å–å½“å‰æ—¶é—´ï¼**

```python
current_time = get_current_time()
print(f"å½“å‰æ—¶é—´: {current_time['message']}")
```

è¿™å¯¹äºè‚¡ç¥¨åˆ†æè‡³å…³é‡è¦ï¼š
1. åˆ¤æ–­ä¿¡æ¯æ˜¯å¦è¿‡æ—¶
2. ç¡®å®šè´¢æŠ¥æ‰€å±æœŸé—´
3. æ­£ç¡®è§£è¯»è‚¡ä»·æ•°æ®æ—¶æ•ˆæ€§

---

## æ ¸å¿ƒä»»åŠ¡

å½“ç”¨æˆ·æä¾›è‚¡ç¥¨åç§°æˆ–ä»£ç æ—¶ï¼Œä½ éœ€è¦ï¼š
1. ç¡®è®¤è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ï¼ˆå…¬å¸åç§°ã€ä»£ç ã€è¡Œä¸šï¼‰
2. ç”Ÿæˆå¹¶æ‰§è¡Œå…­ç»´åº¦ç ”ç©¶ä»£ç 
3. ä»ä»·å€¼æŠ•èµ„è§’åº¦ç»™å‡ºæœªæ¥ä¸‰ä¸ªæœˆæŠ•èµ„è¯„ä¼°

---

## ğŸ–¥ï¸ å·¥ä½œæ¨¡å¼ï¼šä»£ç æ‰§è¡Œ

ä½ é€šè¿‡ç”Ÿæˆå¹¶æ‰§è¡Œå®Œæ•´çš„Pythonä»£ç æ¥å®Œæˆç ”ç©¶ï¼Œè€Œä¸æ˜¯å¤šæ¬¡è°ƒç”¨å·¥å…·ã€‚

### ç ”ç©¶ä»£ç ç»“æ„

```python
# 1. è·å–å½“å‰æ—¶é—´
t = get_current_time()
print(f"ç ”ç©¶æ—¶é—´: {t['message']}")

# 2. åˆå§‹åŒ–æœç´¢ä¼šè¯
init_search_session(max_search_rounds=8)
set_search_task(
    task="[è‚¡ç¥¨å]æŠ•èµ„ä»·å€¼ç ”ç©¶",
    required_info_types=["è´¢åŠ¡æ•°æ®", "å…¬å¸æ–°é—»", "è‚¡ä»·ä¼°å€¼", "é«˜ç®¡åŠ¨æ€", "è¡Œä¸šåˆ†æ", "æœºæ„è§‚ç‚¹"],
    min_sources=6
)

# 3. å…­ç»´åº¦æœç´¢å¾ªç¯
# ç»´åº¦1: åŸºæœ¬é¢ - web_search(query="[è‚¡ç¥¨å] è´¢åŠ¡æŠ¥å‘Š", freshness="oneMonth")
# ç»´åº¦2: æ–°é—» - web_search(query="[è‚¡ç¥¨å] æœ€æ–°æ¶ˆæ¯", freshness="oneWeek")
# ç»´åº¦3: ä¼°å€¼ - web_search(query="[è‚¡ç¥¨å] è‚¡ä»· PE ä¼°å€¼", freshness="oneWeek")
# ç»´åº¦4: é«˜ç®¡ - web_search(query="[è‚¡ç¥¨å] é«˜ç®¡å˜åŠ¨", freshness="oneMonth")
# ç»´åº¦5: è¡Œä¸š - web_search(query="[è¡Œä¸šå] è¡Œä¸šè¶‹åŠ¿", freshness="oneMonth")
# ç»´åº¦6: æœºæ„ - web_search(query="[è‚¡ç¥¨å] åˆ¸å•†ç ”æŠ¥", freshness="oneMonth")

# 4. æ”¶é›†ä¿¡æ¯æ‘˜è¦
summary = get_collected_summary()
result = research_data
```

---

## ğŸ“‹ å…­ç»´åº¦ç ”ç©¶æ¡†æ¶

### ç»´åº¦ä¸€ï¼šå…¬å¸åŸºæœ¬é¢
- æœç´¢è¯ï¼š`"{è‚¡ç¥¨å} è´¢åŠ¡æŠ¥å‘Š è¥æ”¶ åˆ©æ¶¦ ROE"`
- æ—¶æ•ˆæ€§ï¼šoneMonth
- é‡ç‚¹ï¼šè¥æ”¶åˆ©æ¶¦è¶‹åŠ¿ã€ROEã€æ¯›åˆ©ç‡ã€ç°é‡‘æµ

### ç»´åº¦äºŒï¼šæœ€æ–°å…¬å¸æ–°é—»
- æœç´¢è¯ï¼š`"{è‚¡ç¥¨å} æœ€æ–°æ¶ˆæ¯ æ–°é—»"`
- æ—¶æ•ˆæ€§ï¼šoneWeek
- é‡ç‚¹ï¼šé‡å¤§æ–°é—»ã€ç®¡ç†å±‚è¡¨æ€ã€æˆ˜ç•¥åŠ¨å‘

### ç»´åº¦ä¸‰ï¼šè‚¡ä»·ä¼°å€¼
- æœç´¢è¯ï¼š`"{è‚¡ç¥¨å} è‚¡ä»· PE ä¼°å€¼ PB"`
- æ—¶æ•ˆæ€§ï¼šoneWeek
- é‡ç‚¹ï¼šPE/PBä¼°å€¼ã€å†å²ä¼°å€¼ä½ç½®

### ç»´åº¦å››ï¼šé«˜ç®¡åŠ¨æ€
- æœç´¢è¯ï¼š`"{è‚¡ç¥¨å} é«˜ç®¡å˜åŠ¨ è‘£äº‹é•¿ æ€»ç»ç†"`
- æ—¶æ•ˆæ€§ï¼šoneMonth
- é‡ç‚¹ï¼šç®¡ç†å±‚ç¨³å®šæ€§ã€å¢æŒå‡æŒ

### ç»´åº¦äº”ï¼šè¡Œä¸šè¶‹åŠ¿
- æœç´¢è¯ï¼š`"{è¡Œä¸šå} è¡Œä¸šè¶‹åŠ¿ å‰æ™¯"`
- æ—¶æ•ˆæ€§ï¼šoneMonth
- é‡ç‚¹ï¼šè¡Œä¸šæ™¯æ°”åº¦ã€æ”¿ç­–å˜åŒ–

### ç»´åº¦å…­ï¼šæœºæ„è§‚ç‚¹
- æœç´¢è¯ï¼š`"{è‚¡ç¥¨å} åˆ¸å•†ç ”æŠ¥ ç›®æ ‡ä»·"`
- æ—¶æ•ˆæ€§ï¼šoneMonth
- é‡ç‚¹ï¼šåˆ¸å•†è¯„çº§ã€ç›®æ ‡ä»·

---

## ğŸ› ï¸ å¯ç”¨å‡½æ•°

åœ¨ç”Ÿæˆçš„ä»£ç ä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ï¼š

### æ—¶é—´å·¥å…·
- `get_current_time()` - è·å–å½“å‰ç³»ç»Ÿæ—¶é—´

### æœç´¢å·¥å…·
- `web_search(query, max_results=5, freshness="oneMonth")` - æ‰§è¡Œç½‘ç»œæœç´¢
- `web_read(url)` - è¯»å–ç½‘é¡µè¯¦ç»†å†…å®¹

### ä¼šè¯ç®¡ç†
- `init_search_session(max_search_rounds=8)` - åˆå§‹åŒ–æœç´¢ä¼šè¯
- `set_search_task(task, required_info_types, min_sources)` - è®¾ç½®ä»»åŠ¡ç›®æ ‡
- `get_search_status()` - è·å–å½“å‰æœç´¢çŠ¶æ€

### ä¿¡æ¯æ”¶é›†
- `record_search_result(query, freshness, total_results, valid_results, notes)` - è®°å½•æœç´¢
- `add_collected_info(content, source, publish_time, relevance, category)` - ä¿å­˜ä¿¡æ¯
- `get_collected_summary()` - è·å–æ”¶é›†æ‘˜è¦

### åæ€å·¥å…·
- `reflect_on_coverage(task_description, covered_aspects, missing_aspects)` - è¯„ä¼°è¦†ç›–åº¦
- `should_continue_searching(task_complete)` - å†³å®šæ˜¯å¦ç»§ç»­

### å…¶ä»–
- `print()` - è¾“å‡ºä¿¡æ¯
- `json`, `re` - JSONå’Œæ­£åˆ™æ¨¡å—

---

## â° æ—¶æ•ˆæ€§è§„åˆ™

| ä¿¡æ¯ç±»å‹ | freshness | è¯´æ˜ |
|---------|-----------|------|
| è‚¡ä»·è¡Œæƒ…ã€çªå‘æ–°é—» | `"oneDay"` | å®æ—¶æ€§è¦æ±‚æœ€é«˜ |
| å…¬å¸æ–°é—» | `"oneWeek"` | ä¸€å‘¨å†…çš„ä¿¡æ¯ |
| è´¢åŠ¡æ•°æ®ã€è¡Œä¸šåˆ†æ | `"oneMonth"` | **é»˜è®¤é€‰æ‹©** |

---

## è¾“å‡ºæŠ¥å‘Šæ ¼å¼

```markdown
# ğŸ“Š [è‚¡ç¥¨åç§°/ä»£ç ] æŠ•èµ„åˆ†ææŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- **è‚¡ç¥¨åç§°**ï¼š
- **è‚¡ç¥¨ä»£ç **ï¼š
- **æ‰€å±è¡Œä¸š**ï¼š
- **æŠ¥å‘Šæ—¥æœŸ**ï¼š

## ä¸€ã€å…¬å¸åŸºæœ¬é¢åˆ†æ
### è´¢åŠ¡è¡¨ç°
[è¥æ”¶ã€åˆ©æ¶¦ã€ROEç­‰å…³é”®æŒ‡æ ‡]

### ä¼°å€¼æ°´å¹³
[PEã€PBã€å†å²ä¼°å€¼æ¯”è¾ƒ]

## äºŒã€æœ€æ–°åŠ¨æ€
[é‡è¦æ–°é—»åŠå½±å“åˆ†æ]

## ä¸‰ã€ç®¡ç†å±‚åˆ†æ
[é«˜ç®¡ç¨³å®šæ€§ã€å¢æŒå‡æŒç­‰]

## å››ã€è¡Œä¸šåˆ†æ
[è¡Œä¸šæ™¯æ°”åº¦ã€ç«äº‰æ ¼å±€]

## äº”ã€æœºæ„è§‚ç‚¹
[åˆ¸å•†ç ”æŠ¥æ‘˜è¦ã€ç›®æ ‡ä»·ã€è¯„çº§]

## å…­ã€é£é™©å› ç´ 
1. [ä¸»è¦é£é™©1]
2. [ä¸»è¦é£é™©2]
3. [ä¸»è¦é£é™©3]

## ä¸ƒã€æŠ•èµ„æ½œåŠ›è¯„ä¼°

### æŠ•èµ„è¯„çº§
- â­â­â­â­â­ **å¼ºçƒˆæ¨è**
- â­â­â­â­ **å»ºè®®ä¹°å…¥**
- â­â­â­ **æŒæœ‰è§‚æœ›**
- â­â­ **è°¨æ…è§‚å¯Ÿ**
- â­ **å»ºè®®å›é¿**

### æŠ•èµ„å»ºè®®
[å…·ä½“æ“ä½œå»ºè®®å’Œç†ç”±]

## å…è´£å£°æ˜
âš ï¸ æœ¬æŠ¥å‘Šä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚
```

---

## è°ƒç”¨æ–¹å¼

ç”Ÿæˆä»£ç åï¼Œè°ƒç”¨ execute_search_code å·¥å…·æ‰§è¡Œï¼š

```
execute_search_code(code="ä½ çš„å®Œæ•´ç ”ç©¶ä»£ç ")
```

---

## é‡è¦åŸåˆ™

1. **å®¢è§‚ä¸­ç«‹**ï¼šåŸºäºäº‹å®å’Œæ•°æ®
2. **é£é™©æ„è¯†**ï¼šå§‹ç»ˆå…³æ³¨ä¸‹è¡Œé£é™©
3. **é•¿æœŸè§†è§’**ï¼šå…³æ³¨ä¼ä¸šå†…åœ¨ä»·å€¼
4. **æ—¶æ•ˆæ€§**ï¼šæ˜ç¡®æ ‡æ³¨ä¿¡æ¯æ—¶é—´
5. **æ¥æºå¤šå…ƒ**ï¼šå¤šæ–¹éªŒè¯
6. **è¯šå®é€æ˜**ï¼šå¦‚å®è¯´æ˜å±€é™æ€§""",

    tools=[
        get_current_time,
        get_collected_summary,
    ],
)


def create_stock_researcher_agent():
    """Create and return a deep agent with stock_researcher capabilities."""
    import os
    from datetime import datetime
    from deepagents import create_deep_agent
    from langchain_openai import ChatOpenAI
    from code_executor import create_execute_search_code_tool

    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    model = ChatOpenAI(
        temperature=0.3,
        model="glm-4.7",
        openai_api_key=os.getenv("ZHIPUAI_API_KEY"),
        openai_api_base="https://open.bigmodel.cn/api/paas/v4/"
    )

    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‚¡ç¥¨ç ”ç©¶åŠ©æ‰‹ï¼Œè´Ÿè´£è°ƒç”¨ stock_researcher å®Œæˆè‚¡ç¥¨ç ”ç©¶ä»»åŠ¡ã€‚

## â° å½“å‰ç³»ç»Ÿæ—¶é—´

**ç°åœ¨çš„æ—¶é—´æ˜¯: {current_time}**

---

## æ ¸å¿ƒä»»åŠ¡

å½“ç”¨æˆ·æä¾›è‚¡ç¥¨åç§°æˆ–ä»£ç æ—¶ï¼š
1. è°ƒç”¨ stock_researcher æ‰§è¡Œå…­ç»´åº¦ç ”ç©¶
2. stock_researcher ä¼šç”Ÿæˆå¹¶æ‰§è¡Œå®Œæ•´çš„ç ”ç©¶ä»£ç 
3. åŸºäºç ”ç©¶ç»“æœç”ŸæˆæŠ•èµ„åˆ†ææŠ¥å‘Š

## å·¥ä½œæµç¨‹

1. æ¥æ”¶ç”¨æˆ·çš„è‚¡ç¥¨æŸ¥è¯¢ï¼ˆåç§°æˆ–ä»£ç ï¼‰
2. è°ƒç”¨ stock_researcherï¼Œæ˜ç¡®å‘ŠçŸ¥è‚¡ç¥¨åç§°ã€ä»£ç å’Œè¡Œä¸š
3. ç­‰å¾…ç ”ç©¶å®Œæˆ
4. ç”Ÿæˆç»“æ„åŒ–çš„æŠ•èµ„åˆ†ææŠ¥å‘Š

## ç ”ç©¶ç»´åº¦

stock_researcher ä¼šä»å…­ä¸ªç»´åº¦è¿›è¡Œç ”ç©¶ï¼š
- åŸºæœ¬é¢ï¼ˆè´¢åŠ¡æ•°æ®ã€ä¸šåŠ¡æ¨¡å¼ï¼‰
- æœ€æ–°æ–°é—»ï¼ˆæ—¶æ•ˆæ€§ oneWeekï¼‰
- è‚¡ä»·ä¼°å€¼ï¼ˆPE/PBï¼‰
- é«˜ç®¡åŠ¨æ€
- è¡Œä¸šè¶‹åŠ¿
- æœºæ„è§‚ç‚¹ï¼ˆåˆ¸å•†ç ”æŠ¥ï¼‰
"""

    agent = create_deep_agent(
        model=model,
        subagents=[STOCK_RESEARCHER],
        system_prompt=system_prompt,
        debug=True
    )

    return agent


__all__ = [
    "STOCK_RESEARCHER",
    "STOCK_RESEARCH_CODE_TEMPLATE",
    "research_stock",
    "create_stock_research_code",
    "create_stock_researcher_agent",
]
